config:
  modules:
    http_2xx:
      prober: http
      timeout: 5s
      http:
        valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
        follow_redirects: true
        preferred_ip_protocol: "ip4"
        tls_config:
          insecure_skip_verify: true
    http_redirect_location_set:
      prober: http
      timeout: 5s
      http:
        method: GET
        valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
        no_follow_redirects: true
        preferred_ip_protocol: "ip4"
        tls_config:
          insecure_skip_verify: false
        valid_status_codes: [302]
        fail_if_header_not_matches:
          - header: Location
            regexp: "^.+$"
    http_2xx_content_ok:
      prober: http
      timeout: 8s
      http:
        method: GET
        valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
        no_follow_redirects: false
        preferred_ip_protocol: "ip4"
        tls_config:
          insecure_skip_verify: true
        fail_if_body_not_matches_regexp:
          - "OK"

serviceMonitor:
  selfMonitor:
    enabled: true
    port: "http"

  enabled: true

  targets:
    - name: pin.bastion.metasonic.cloudical.in
      url: https://pin.bastion.metasonic.cloudical.in:8090
      module: http_2xx
      additionalMetricsRelabels:
        customer: "Cloudical"
        environment: "pin"
        host: pin.bastion.metasonic.cloudical.in
        proxy: true

prometheusRule:
  enabled: true
  additionalLabels:
    role: alert-rules
    cluster: cloud-clops
  namespace: "kube-prometheus-stack"
  rules:
    - alert: EndpointDown
      expr: probe_success == 0
      # If a service is down for 45 seconds and more something **might** be wrong or just a short blip,
      # so let's cause a warning first for "documentation purposes".
      for: 45s
      labels:
        severity: "warning"
      annotations:
        # https://github.com/helm/helm/issues/2798#issuecomment-467319526
        summary: "Endpoint {{`{{ $labels.instance }}`}} down (45s)"
    - alert: EndpointDown
      expr: probe_success == 0
      # If a service is down for more 2-3+ minutes something is definitely wrong
      # either the service is down / broken, blackbox exporter has some issues,
      # the K8S clusters internet access has issues or some other problem (probably
      # network related issue).
      for: 180s
      labels:
        severity: "critical"
      annotations:
        summary: "Endpoint {{`{{ $labels.instance }}`}} down (3 minutes)"
    - alert: CertificateExpiry
      # 14 Tage Warnung
      expr: (probe_ssl_earliest_cert_expiry - (604800 * 2)) <= time()
      labels:
        severity: "warning"
      annotations:
        summary: "Certificate {{`{{ $labels.instance }}`}} expires in less than 14 days."
    - alert: CertificateExpiry
      # 7 Tage Warnung
      expr: (probe_ssl_earliest_cert_expiry - 604800) <= time()
      labels:
        severity: "critical"
      annotations:
        summary: "Certificate {{`{{ $labels.instance }}`}} expires in less than 7 days."
    - alert: LokiExessiveMemoryUsage
      labels:
        severity: "warning"
      expr: count(container_memory_working_set_bytes{namespace="monitoring", container="loki"} > 5000000000) > 0
      annotations:
        summary: "Loki is using exessive memory ressources"
    - alert: LokiExessiveMemoryUsage
      labels:
        severity: "critical"
      expr: count(container_memory_working_set_bytes{namespace="monitoring", container="loki"} > 10000000000) > 0
      annotations:
        summary: "Loki is using extremely exessive memory ressources"
