clusterSize: 0

autoSetup:
  enabled: false
  image:
    repository: curlimages/curl
    tag: latest
    pullPolicy: Always
  defaultDatabases:
    - conversations
    - credentials

adminPassword: WpoTGkBvOvkLd63nKzu8jGOymzLq07INexdPqwvgONP16qZT7IsL7q8O4FurTzoc
cookieAuthSecret: k9qrqkAVmv1nieahaxUWZsEpFaNS5AGf5rPYANtgRZLq2VwRQswliwcJB0pJEW0f

persistentVolume:
  enabled: true

## The CouchDB image
image:
  repository: couchdb
  tag: 3.3.2
  pullPolicy: IfNotPresent

## Experimental integration with Lucene-powered fulltext search
searchImage:
  repository: kocolosk/couchdb-search
  tag: 0.2.0
  pullPolicy: IfNotPresent

# -- Flip this to flag to include the Search container in each Pod
enableSearch: false

initImage:
  repository: busybox
  tag: latest
  pullPolicy: Always

## CouchDB is happy to spin up cluster nodes in parallel, but if you encounter
## problems you can try setting podManagementPolicy to the StatefulSet default
## `OrderedReady`
podManagementPolicy: Parallel

## To better tolerate Node failures, we can prevent Kubernetes scheduler from
## assigning more than one Pod of CouchDB StatefulSet per Node using podAntiAffinity.
affinity: {}
  # podAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     - labelSelector:
  #         matchExpressions:
  #           - key: "app"
  #             operator: In
  #             values:
  #             - couchdb
  #       topologyKey: "kubernetes.io/hostname"

## To control how Pods are spread across your cluster among failure-domains such as regions,
## zones, nodes, and other user-defined topology domains use topologySpreadConstraints.
topologySpreadConstraints: {}
  # topologySpreadConstraints:
  #   - maxSkew: 1
  #     topologyKey: "topology.kubernetes.io/zone"
  #     whenUnsatisfiable: ScheduleAnyway
  #     labelSelector:
  #       matchLabels:
  #         app: couchdb

## Optional pod labels
labels: {}

## Optional pod annotations
annotations: {}

## Optional tolerations
tolerations: []

## A StatefulSet requires a headless Service to establish the stable network
## identities of the Pods, and that Service is created automatically by this
## chart without any additional configuration. The Service block below refers
## to a second Service that governs how clients connect to the CouchDB cluster.
service:
  annotations: {}
  enabled: true
  type: ClusterIP
  externalPort: 5984
  targetPort: 5984
  labels: {}

## An Ingress resource can provide name-based virtual hosting and TLS
## termination among other things for CouchDB deployments which are accessed
## from outside the Kubernetes cluster.
## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: false
  className: nginx
  hosts:
    - couchdb.ip-oots-service.dev.efa.publicplan.cloud
  path: /
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  tls:
    # Secrets must be manually created in the namespace.
    # - secretName: chart-example-tls
    #   hosts:
    #     - chart-example.local

## Optional resource requests and limits for the CouchDB container
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
resources: {}
  # requests:
  #  cpu: 100m
  #  memory: 128Mi
  # limits:
  #  cpu: 56
  #  memory: 256Gi

## Optional resource requests and limits for the CouchDB init container
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
initResources: {}
  # requests:
  #  cpu: 100m
  #  memory: 128Mi
  # limits:
  #  cpu: 500m
  #  memory: 128Mi

# -- erlangFlags is a map that is passed to the Erlang VM as flags using the
# ERL_FLAGS env. The `name` flag is required to establish connectivity
# between cluster nodes.
# ref: http://erlang.org/doc/man/erl.html#init_flags
erlangFlags:
  name: couchdb
  # Older versions of the official CouchDB image (anything prior to 3.2.1)
  # do not act on the COUCHDB_ERLANG_COOKIE environment variable, so if you
  # want to cluster these deployments it's necessary to pass in a cookie here
  setcookie: aQwph0AxeRtNzl4w1xVAM0NO8pEhGUjzoxbGlI74yMaRS33di0rkj1XGQdxsVOaJ

# -- couchdbConfig will override default CouchDB configuration settings.
# The contents of this map are reformatted into a .ini file laid down
# by a ConfigMap object.
# ref: http://docs.couchdb.org/en/latest/config/index.html
couchdbConfig:
  couchdb:
   uuid: 47492869-4fad-4d50-ac95-cb55d08abc1e # Unique identifier for this CouchDB server instance
  # cluster:
  #   q: 8 # Create 8 shards for each database
  chttpd:
    bind_address: any
    # chttpd.require_valid_user disables all the anonymous requests to the port
    # 5984 when is set to true.
    require_valid_user: false
  # required to use Fauxton if chttpd.require_valid_user is set to true
  # httpd:
  #   WWW-Authenticate: "Basic realm=\"administrator\""

# Kubernetes local cluster domain.
# This is used to generate FQDNs for peers when joining the CouchDB cluster.
dns:
  clusterDomainSuffix: cluster.local

## Configure liveness and readiness probe values
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
livenessProbe:
  enabled: true
  failureThreshold: 3
  initialDelaySeconds: 0
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1
readinessProbe:
  enabled: true
  failureThreshold: 3
  initialDelaySeconds: 0
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1

# Control an optional pod disruption budget
podDisruptionBudget:
  # toggle creation of pod disruption budget, disabled by default
  enabled: false
  # minAvailable: 1
  maxUnavailable: 1

# CouchDB 3.2.0 adds in a metrics endpoint on the path `/_node/_local/_prometheus`.
# Optionally, a standalone, unauthenticated port can be exposed for these metrics.
prometheusPort:
  enabled: false
  bind_address: "0.0.0.0"
  port: 17986

# Configure arbitrary sidecar containers for CouchDB pods created by the
# StatefulSet
sidecars: {}
  # - name: foo
  #   image: "busybox"
  #   imagePullPolicy: IfNotPresent
  #   resources:
  #     requests:
  #       cpu: "0.1"
  #       memory: 10Mi
  #   command: ['echo "foo";']
  #   volumeMounts:
  #     - name: database-storage
  #       mountPath: /opt/couchdb/data/

# Placement manager to annotate each document in the nodes DB with "zone" attribute
# recording the zone where node has been scheduled
# Ref: https://docs.couchdb.org/en/stable/cluster/sharding.html#specifying-database-placement
placementConfig:
  enabled: false
  image:
    repository: caligrafix/couchdb-autoscaler-placement-manager
    tag: 0.1.0

